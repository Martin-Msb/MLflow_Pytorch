{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e06fdc-5bb9-4199-afaa-9aba2ff02f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.13.0.post200\n",
      "torchvision: 0.14.0\n",
      "sklearn: 1.1.3\n",
      "MLFlow: 2.2.1\n",
      "Numpy: 1.23.5\n",
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"PyTorch: {}\".format(torch.__version__))\n",
    "print(\"torchvision: {}\".format(torchvision.__version__))\n",
    "print(\"sklearn: {}\".format(sklearn.__version__))\n",
    "print(\"MLFlow: {}\".format(mlflow.__version__))\n",
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece93a78-4836-4eb5-a628-dd5b8f5005b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fd5073-3681-42b1-9eaf-ded95d281435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a90dc0472149aaa9e93fd227d13511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2a01ff3130462c99608f6fb4d33fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7de897ef944e2ea0a84737087b84c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a4f2de5d7c47a980ab6e5f130b9c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c68f94-ffc7-43da-8ebf-d6318e14c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_set.data, train_set.targets\n",
    "x_test, y_test = test_set.data, test_set.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ee9555-05ac-4602-926c-f817796cdc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1], x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1], x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c772f345-2c5a-4c89-badc-0b07d8eab37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to transform to categorical, like to_categorical() in Keras\n",
    "def to_one_hot(num_classes, labels):\n",
    "    one_hot = torch.zeros(([labels.shape[0], num_classes]))\n",
    "    for f in range(len(labels)):\n",
    "        one_hot[f][labels[f]] = 1\n",
    "        \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e9c1c38-bf96-4958-a82b-a5714b2695be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert y sets into one-hot encoded format using the function\n",
    "y_train = to_one_hot(num_classes, y_train)\n",
    "y_test = to_one_hot(num_classes, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c86c72ee-5cd3-4c64-997d-05976ada789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "x_train: torch.Size([60000, 1, 28, 28])\n",
      "y_train: torch.Size([60000, 10])\n",
      "x_test: torch.Size([10000, 1, 28, 28])\n",
      "y_test: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes\")\n",
    "print(\"x_train: {}\\ny_train: {}\".format(x_train.shape,\n",
    "y_train.shape))\n",
    "print(\"x_test: {}\\ny_test: {}\".format(x_test.shape,\n",
    "y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a15ae02f-619f-47b0-be59-cb8f4dea60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        # IN 1x28x28 OUT 16x14x14\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        # IN 16x14x14 OUT 32x6x6\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        # IN 32x6x6 OUT 64x2x2\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        # IN 64x2x2 OUT 256\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.dense2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.dense3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dense2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dense3(x)\n",
    "        x = nn.Softmax()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1517c8b6-f2c6-4636-8856-12d55e251acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d75e545-d7f1-4479-b1b2-d59da4c213be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(x_train,y_train)\n",
    "train_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fd47a0d-ec6f-4f62-8957-bb5fd223a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch_Num 0 Loss 0.3757890462875366\n",
      "Epoch 0 Batch_Num 1 Loss 0.3312697112560272\n",
      "Epoch 0 Batch_Num 2 Loss 0.31889110803604126\n",
      "Epoch 0 Batch_Num 3 Loss 0.29748207330703735\n",
      "Epoch 0 Batch_Num 4 Loss 0.2898063063621521\n",
      "Epoch 0 Batch_Num 5 Loss 0.26584184169769287\n",
      "Epoch 0 Batch_Num 6 Loss 0.2531784176826477\n",
      "Epoch 0 Batch_Num 7 Loss 0.2369346171617508\n",
      "Epoch 0 Batch_Num 8 Loss 0.21736092865467072\n",
      "Epoch 0 Batch_Num 9 Loss 0.1907939910888672\n",
      "Epoch 0 Batch_Num 10 Loss 0.1657032072544098\n",
      "Epoch 0 Batch_Num 11 Loss 0.15756025910377502\n",
      "Epoch 0 Batch_Num 12 Loss 0.1422877460718155\n",
      "Epoch 0 Batch_Num 13 Loss 0.14614112675189972\n",
      "Epoch 0 Batch_Num 14 Loss 0.13269813358783722\n",
      "Epoch 0 Batch_Num 15 Loss 0.11394134908914566\n",
      "Epoch 0 Batch_Num 16 Loss 0.10910794883966446\n",
      "Epoch 0 Batch_Num 17 Loss 0.09012476354837418\n",
      "Epoch 0 Batch_Num 18 Loss 0.12695950269699097\n",
      "Epoch 0 Batch_Num 19 Loss 0.09045200794935226\n",
      "Epoch 0 Batch_Num 20 Loss 0.0812843069434166\n",
      "Epoch 0 Batch_Num 21 Loss 0.07577208429574966\n",
      "Epoch 0 Batch_Num 22 Loss 0.10058029741048813\n",
      "Epoch 0 Batch_Num 23 Loss 0.05361798405647278\n",
      "Epoch 0 Batch_Num 24 Loss 0.046510640531778336\n",
      "Epoch 0 Batch_Num 25 Loss 0.057470258325338364\n",
      "Epoch 0 Batch_Num 26 Loss 0.07994984835386276\n",
      "Epoch 0 Batch_Num 27 Loss 0.08952035754919052\n",
      "Epoch 0 Batch_Num 28 Loss 0.09623992443084717\n",
      "Epoch 0 Batch_Num 29 Loss 0.06828468292951584\n",
      "Epoch 0 Batch_Num 30 Loss 0.07209142297506332\n",
      "Epoch 0 Batch_Num 31 Loss 0.060661233961582184\n",
      "Epoch 0 Batch_Num 32 Loss 0.07870346307754517\n",
      "Epoch 0 Batch_Num 33 Loss 0.07252227514982224\n",
      "Epoch 0 Batch_Num 34 Loss 0.11787448078393936\n",
      "Epoch 0 Batch_Num 35 Loss 0.05111042782664299\n",
      "Epoch 0 Batch_Num 36 Loss 0.07027699053287506\n",
      "Epoch 0 Batch_Num 37 Loss 0.05744211748242378\n",
      "Epoch 0 Batch_Num 38 Loss 0.06239185482263565\n",
      "Epoch 0 Batch_Num 39 Loss 0.08218180388212204\n",
      "Epoch 0 Batch_Num 40 Loss 0.04985416680574417\n",
      "Epoch 0 Batch_Num 41 Loss 0.05247443541884422\n",
      "Epoch 0 Batch_Num 42 Loss 0.06449186056852341\n",
      "Epoch 0 Batch_Num 43 Loss 0.06886470317840576\n",
      "Epoch 0 Batch_Num 44 Loss 0.057227104902267456\n",
      "Epoch 0 Batch_Num 45 Loss 0.07495187968015671\n",
      "Epoch 0 Batch_Num 46 Loss 0.066213458776474\n",
      "Epoch 0 Batch_Num 47 Loss 0.06627387553453445\n",
      "Epoch 0 Batch_Num 48 Loss 0.07257377356290817\n",
      "Epoch 0 Batch_Num 49 Loss 0.08400209993124008\n",
      "Epoch 0 Batch_Num 50 Loss 0.07288135588169098\n",
      "Epoch 0 Batch_Num 51 Loss 0.06427526473999023\n",
      "Epoch 0 Batch_Num 52 Loss 0.061254777014255524\n",
      "Epoch 0 Batch_Num 53 Loss 0.056003719568252563\n",
      "Epoch 0 Batch_Num 54 Loss 0.06480992585420609\n",
      "Epoch 0 Batch_Num 55 Loss 0.06206327676773071\n",
      "Epoch 0 Batch_Num 56 Loss 0.06735645234584808\n",
      "Epoch 0 Batch_Num 57 Loss 0.08998022228479385\n",
      "Epoch 0 Batch_Num 58 Loss 0.04342646524310112\n",
      "Epoch 0 Batch_Num 59 Loss 0.056119199842214584\n",
      "Epoch 0 Batch_Num 60 Loss 0.038545459508895874\n",
      "Epoch 0 Batch_Num 61 Loss 0.0525500550866127\n",
      "Epoch 0 Batch_Num 62 Loss 0.06157630681991577\n",
      "Epoch 0 Batch_Num 63 Loss 0.03070085681974888\n",
      "Epoch 0 Batch_Num 64 Loss 0.03516184166073799\n",
      "Epoch 0 Batch_Num 65 Loss 0.049412112683057785\n",
      "Epoch 0 Batch_Num 66 Loss 0.050717856734991074\n",
      "Epoch 0 Batch_Num 67 Loss 0.036946721374988556\n",
      "Epoch 0 Batch_Num 68 Loss 0.052230384200811386\n",
      "Epoch 0 Batch_Num 69 Loss 0.05060242488980293\n",
      "Epoch 0 Batch_Num 70 Loss 0.026400268077850342\n",
      "Epoch 0 Batch_Num 71 Loss 0.0310552716255188\n",
      "Epoch 0 Batch_Num 72 Loss 0.03479914739727974\n",
      "Epoch 0 Batch_Num 73 Loss 0.03619939461350441\n",
      "Epoch 0 Batch_Num 74 Loss 0.031231379136443138\n",
      "Epoch 0 Batch_Num 75 Loss 0.042735446244478226\n",
      "Epoch 0 Batch_Num 76 Loss 0.032748736441135406\n",
      "Epoch 0 Batch_Num 77 Loss 0.02713884972035885\n",
      "Epoch 0 Batch_Num 78 Loss 0.06696385890245438\n",
      "Epoch 0 Batch_Num 79 Loss 0.04454904422163963\n",
      "Epoch 0 Batch_Num 80 Loss 0.05291273817420006\n",
      "Epoch 0 Batch_Num 81 Loss 0.04980763792991638\n",
      "Epoch 0 Batch_Num 82 Loss 0.03545692190527916\n",
      "Epoch 0 Batch_Num 83 Loss 0.027462396770715714\n",
      "Epoch 0 Batch_Num 84 Loss 0.03939566761255264\n",
      "Epoch 0 Batch_Num 85 Loss 0.0249142125248909\n",
      "Epoch 0 Batch_Num 86 Loss 0.05085024982690811\n",
      "Epoch 0 Batch_Num 87 Loss 0.0345541350543499\n",
      "Epoch 0 Batch_Num 88 Loss 0.040211766958236694\n",
      "Epoch 0 Batch_Num 89 Loss 0.018429895862936974\n",
      "Epoch 0 Batch_Num 90 Loss 0.025275593623518944\n",
      "Epoch 0 Batch_Num 91 Loss 0.030859339982271194\n",
      "Epoch 0 Batch_Num 92 Loss 0.035923395305871964\n",
      "Epoch 0 Batch_Num 93 Loss 0.0357542410492897\n",
      "Epoch 0 Batch_Num 94 Loss 0.03630511090159416\n",
      "Epoch 0 Batch_Num 95 Loss 0.032607413828372955\n",
      "Epoch 0 Batch_Num 96 Loss 0.05517349764704704\n",
      "Epoch 0 Batch_Num 97 Loss 0.027770480141043663\n",
      "Epoch 0 Batch_Num 98 Loss 0.03529030457139015\n",
      "Epoch 0 Batch_Num 99 Loss 0.01990855112671852\n",
      "Epoch 0 Batch_Num 100 Loss 0.03074938990175724\n",
      "Epoch 0 Batch_Num 101 Loss 0.02887796238064766\n",
      "Epoch 0 Batch_Num 102 Loss 0.020685454830527306\n",
      "Epoch 0 Batch_Num 103 Loss 0.04340556636452675\n",
      "Epoch 0 Batch_Num 104 Loss 0.06375955045223236\n",
      "Epoch 0 Batch_Num 105 Loss 0.027349574491381645\n",
      "Epoch 0 Batch_Num 106 Loss 0.03175455331802368\n",
      "Epoch 0 Batch_Num 107 Loss 0.03286358341574669\n",
      "Epoch 0 Batch_Num 108 Loss 0.030278874561190605\n",
      "Epoch 0 Batch_Num 109 Loss 0.025583138689398766\n",
      "Epoch 0 Batch_Num 110 Loss 0.04111921787261963\n",
      "Epoch 0 Batch_Num 111 Loss 0.03522124141454697\n",
      "Epoch 0 Batch_Num 112 Loss 0.023886285722255707\n",
      "Epoch 0 Batch_Num 113 Loss 0.030873075127601624\n",
      "Epoch 0 Batch_Num 114 Loss 0.0346405990421772\n",
      "Epoch 0 Batch_Num 115 Loss 0.02618005871772766\n",
      "Epoch 0 Batch_Num 116 Loss 0.0390869677066803\n",
      "Epoch 0 Batch_Num 117 Loss 0.035559289157390594\n",
      "Epoch 0 Batch_Num 118 Loss 0.013795062899589539\n",
      "Epoch 0 Batch_Num 119 Loss 0.04667791351675987\n",
      "Epoch 0 Batch_Num 120 Loss 0.028940195217728615\n",
      "Epoch 0 Batch_Num 121 Loss 0.024306876584887505\n",
      "Epoch 0 Batch_Num 122 Loss 0.04871741682291031\n",
      "Epoch 0 Batch_Num 123 Loss 0.03949679061770439\n",
      "Epoch 0 Batch_Num 124 Loss 0.028901590034365654\n",
      "Epoch 0 Batch_Num 125 Loss 0.03720281645655632\n",
      "Epoch 0 Batch_Num 126 Loss 0.04233628511428833\n",
      "Epoch 0 Batch_Num 127 Loss 0.01898026652634144\n",
      "Epoch 0 Batch_Num 128 Loss 0.03201674297451973\n",
      "Epoch 0 Batch_Num 129 Loss 0.027470601722598076\n",
      "Epoch 0 Batch_Num 130 Loss 0.03335389122366905\n",
      "Epoch 0 Batch_Num 131 Loss 0.024177102372050285\n",
      "Epoch 0 Batch_Num 132 Loss 0.016807544976472855\n",
      "Epoch 0 Batch_Num 133 Loss 0.014106899499893188\n",
      "Epoch 0 Batch_Num 134 Loss 0.03774518147110939\n",
      "Epoch 0 Batch_Num 135 Loss 0.03016563318669796\n",
      "Epoch 0 Batch_Num 136 Loss 0.027967143803834915\n",
      "Epoch 0 Batch_Num 137 Loss 0.036048807203769684\n",
      "Epoch 0 Batch_Num 138 Loss 0.02140958420932293\n",
      "Epoch 0 Batch_Num 139 Loss 0.020962439477443695\n",
      "Epoch 0 Batch_Num 140 Loss 0.03169966861605644\n",
      "Epoch 0 Batch_Num 141 Loss 0.02575979195535183\n",
      "Epoch 0 Batch_Num 142 Loss 0.03310651332139969\n",
      "Epoch 0 Batch_Num 143 Loss 0.02486216463148594\n",
      "Epoch 0 Batch_Num 144 Loss 0.022830013185739517\n",
      "Epoch 0 Batch_Num 145 Loss 0.024081462994217873\n",
      "Epoch 0 Batch_Num 146 Loss 0.044981375336647034\n",
      "Epoch 0 Batch_Num 147 Loss 0.03053504228591919\n",
      "Epoch 0 Batch_Num 148 Loss 0.01942591741681099\n",
      "Epoch 0 Batch_Num 149 Loss 0.028578907251358032\n",
      "Epoch 0 Batch_Num 150 Loss 0.028184575960040092\n",
      "Epoch 0 Batch_Num 151 Loss 0.024667587131261826\n",
      "Epoch 0 Batch_Num 152 Loss 0.014598192647099495\n",
      "Epoch 0 Batch_Num 153 Loss 0.04841858148574829\n",
      "Epoch 0 Batch_Num 154 Loss 0.029084110632538795\n",
      "Epoch 0 Batch_Num 155 Loss 0.02250480465590954\n",
      "Epoch 0 Batch_Num 156 Loss 0.020324820652604103\n",
      "Epoch 0 Batch_Num 157 Loss 0.02637084387242794\n",
      "Epoch 0 Batch_Num 158 Loss 0.018434202298521996\n",
      "Epoch 0 Batch_Num 159 Loss 0.017476148903369904\n",
      "Epoch 0 Batch_Num 160 Loss 0.02687949500977993\n",
      "Epoch 0 Batch_Num 161 Loss 0.040794651955366135\n",
      "Epoch 0 Batch_Num 162 Loss 0.022763367742300034\n",
      "Epoch 0 Batch_Num 163 Loss 0.031423866748809814\n",
      "Epoch 0 Batch_Num 164 Loss 0.02743004821240902\n",
      "Epoch 0 Batch_Num 165 Loss 0.022071735933423042\n",
      "Epoch 0 Batch_Num 166 Loss 0.031783998012542725\n",
      "Epoch 0 Batch_Num 167 Loss 0.035132091492414474\n",
      "Epoch 0 Batch_Num 168 Loss 0.02933599054813385\n",
      "Epoch 0 Batch_Num 169 Loss 0.015002923086285591\n",
      "Epoch 0 Batch_Num 170 Loss 0.021039972081780434\n",
      "Epoch 0 Batch_Num 171 Loss 0.022820794954895973\n",
      "Epoch 0 Batch_Num 172 Loss 0.04104803875088692\n",
      "Epoch 0 Batch_Num 173 Loss 0.0423823706805706\n",
      "Epoch 0 Batch_Num 174 Loss 0.012522962875664234\n",
      "Epoch 0 Batch_Num 175 Loss 0.03530322387814522\n",
      "Epoch 0 Batch_Num 176 Loss 0.02847438119351864\n",
      "Epoch 0 Batch_Num 177 Loss 0.033073022961616516\n",
      "Epoch 0 Batch_Num 178 Loss 0.03284381702542305\n",
      "Epoch 0 Batch_Num 179 Loss 0.0378158800303936\n",
      "Epoch 0 Batch_Num 180 Loss 0.04746459051966667\n",
      "Epoch 0 Batch_Num 181 Loss 0.024801744148135185\n",
      "Epoch 0 Batch_Num 182 Loss 0.018132518976926804\n",
      "Epoch 0 Batch_Num 183 Loss 0.022663723677396774\n",
      "Epoch 0 Batch_Num 184 Loss 0.03125383332371712\n",
      "Epoch 0 Batch_Num 185 Loss 0.037108954042196274\n",
      "Epoch 0 Batch_Num 186 Loss 0.021994715556502342\n",
      "Epoch 0 Batch_Num 187 Loss 0.02791365422308445\n",
      "Epoch 0 Batch_Num 188 Loss 0.01877177134156227\n",
      "Epoch 0 Batch_Num 189 Loss 0.016009313985705376\n",
      "Epoch 0 Batch_Num 190 Loss 0.010645898059010506\n",
      "Epoch 0 Batch_Num 191 Loss 0.04717164859175682\n",
      "Epoch 0 Batch_Num 192 Loss 0.023351967334747314\n",
      "Epoch 0 Batch_Num 193 Loss 0.036977630108594894\n",
      "Epoch 0 Batch_Num 194 Loss 0.03075387515127659\n",
      "Epoch 0 Batch_Num 195 Loss 0.019001206383109093\n",
      "Epoch 0 Batch_Num 196 Loss 0.036871880292892456\n",
      "Epoch 0 Batch_Num 197 Loss 0.02435932867228985\n",
      "Epoch 0 Batch_Num 198 Loss 0.029027072712779045\n",
      "Epoch 0 Batch_Num 199 Loss 0.013275638222694397\n",
      "Epoch 0 Batch_Num 200 Loss 0.02132825180888176\n",
      "Epoch 0 Batch_Num 201 Loss 0.015192940831184387\n",
      "Epoch 0 Batch_Num 202 Loss 0.01692732609808445\n",
      "Epoch 0 Batch_Num 203 Loss 0.04263870045542717\n",
      "Epoch 0 Batch_Num 204 Loss 0.01675761118531227\n",
      "Epoch 0 Batch_Num 205 Loss 0.01871313713490963\n",
      "Epoch 0 Batch_Num 206 Loss 0.04989853501319885\n",
      "Epoch 0 Batch_Num 207 Loss 0.021520813927054405\n",
      "Epoch 0 Batch_Num 208 Loss 0.01239411998540163\n",
      "Epoch 0 Batch_Num 209 Loss 0.03555314615368843\n",
      "Epoch 0 Batch_Num 210 Loss 0.037707190960645676\n",
      "Epoch 0 Batch_Num 211 Loss 0.019765224307775497\n",
      "Epoch 0 Batch_Num 212 Loss 0.015893010422587395\n",
      "Epoch 0 Batch_Num 213 Loss 0.01668543927371502\n",
      "Epoch 0 Batch_Num 214 Loss 0.0420939140021801\n",
      "Epoch 0 Batch_Num 215 Loss 0.019601548090577126\n",
      "Epoch 0 Batch_Num 216 Loss 0.029109884053468704\n",
      "Epoch 0 Batch_Num 217 Loss 0.019964169710874557\n",
      "Epoch 0 Batch_Num 218 Loss 0.01594085805118084\n",
      "Epoch 0 Batch_Num 219 Loss 0.02253141812980175\n",
      "Epoch 0 Batch_Num 220 Loss 0.030922288075089455\n",
      "Epoch 0 Batch_Num 221 Loss 0.035360775887966156\n",
      "Epoch 0 Batch_Num 222 Loss 0.01696476712822914\n",
      "Epoch 0 Batch_Num 223 Loss 0.013971135020256042\n",
      "Epoch 0 Batch_Num 224 Loss 0.01222334336489439\n",
      "Epoch 0 Batch_Num 225 Loss 0.02266743592917919\n",
      "Epoch 0 Batch_Num 226 Loss 0.012504592537879944\n",
      "Epoch 0 Batch_Num 227 Loss 0.004142057150602341\n",
      "Epoch 0 Batch_Num 228 Loss 0.00993204116821289\n",
      "Epoch 0 Batch_Num 229 Loss 0.018749771639704704\n",
      "Epoch 0 Batch_Num 230 Loss 0.0011566294124349952\n",
      "Epoch 0 Batch_Num 231 Loss 0.01225682906806469\n",
      "Epoch 0 Batch_Num 232 Loss 0.005935527849942446\n",
      "Epoch 0 Batch_Num 233 Loss 0.029454529285430908\n",
      "Epoch 0 Batch_Num 234 Loss 0.031992338597774506\n",
      "Epoch 1 Batch_Num 0 Loss 0.022223209962248802\n",
      "Epoch 1 Batch_Num 1 Loss 0.018926244229078293\n",
      "Epoch 1 Batch_Num 2 Loss 0.01616584323346615\n",
      "Epoch 1 Batch_Num 3 Loss 0.03426824137568474\n",
      "Epoch 1 Batch_Num 4 Loss 0.026479298248887062\n",
      "Epoch 1 Batch_Num 5 Loss 0.017787709832191467\n",
      "Epoch 1 Batch_Num 6 Loss 0.018518298864364624\n",
      "Epoch 1 Batch_Num 7 Loss 0.011573153547942638\n",
      "Epoch 1 Batch_Num 8 Loss 0.011984485201537609\n",
      "Epoch 1 Batch_Num 9 Loss 0.012333798222243786\n",
      "Epoch 1 Batch_Num 10 Loss 0.02417933940887451\n",
      "Epoch 1 Batch_Num 11 Loss 0.015229187905788422\n",
      "Epoch 1 Batch_Num 12 Loss 0.014764562249183655\n",
      "Epoch 1 Batch_Num 13 Loss 0.018744751811027527\n",
      "Epoch 1 Batch_Num 14 Loss 0.022380676120519638\n",
      "Epoch 1 Batch_Num 15 Loss 0.0142149543389678\n",
      "Epoch 1 Batch_Num 16 Loss 0.010759808123111725\n",
      "Epoch 1 Batch_Num 17 Loss 0.0144125921651721\n",
      "Epoch 1 Batch_Num 18 Loss 0.019498467445373535\n",
      "Epoch 1 Batch_Num 19 Loss 0.022262318059802055\n",
      "Epoch 1 Batch_Num 20 Loss 0.02617967128753662\n",
      "Epoch 1 Batch_Num 21 Loss 0.009952233172953129\n",
      "Epoch 1 Batch_Num 22 Loss 0.026461422443389893\n",
      "Epoch 1 Batch_Num 23 Loss 0.013606391847133636\n",
      "Epoch 1 Batch_Num 24 Loss 0.016689708456397057\n",
      "Epoch 1 Batch_Num 25 Loss 0.013263215310871601\n",
      "Epoch 1 Batch_Num 26 Loss 0.02246720902621746\n",
      "Epoch 1 Batch_Num 27 Loss 0.016515258699655533\n",
      "Epoch 1 Batch_Num 28 Loss 0.026073560118675232\n",
      "Epoch 1 Batch_Num 29 Loss 0.011815962381660938\n",
      "Epoch 1 Batch_Num 30 Loss 0.02557850442826748\n",
      "Epoch 1 Batch_Num 31 Loss 0.014964312314987183\n",
      "Epoch 1 Batch_Num 32 Loss 0.025547310709953308\n",
      "Epoch 1 Batch_Num 33 Loss 0.01570458710193634\n",
      "Epoch 1 Batch_Num 34 Loss 0.04147381708025932\n",
      "Epoch 1 Batch_Num 35 Loss 0.013192841783165932\n",
      "Epoch 1 Batch_Num 36 Loss 0.02451191283762455\n",
      "Epoch 1 Batch_Num 37 Loss 0.010866383090615273\n",
      "Epoch 1 Batch_Num 38 Loss 0.01191424485296011\n",
      "Epoch 1 Batch_Num 39 Loss 0.027206284925341606\n",
      "Epoch 1 Batch_Num 40 Loss 0.02972562611103058\n",
      "Epoch 1 Batch_Num 41 Loss 0.009684179909527302\n",
      "Epoch 1 Batch_Num 42 Loss 0.020398229360580444\n",
      "Epoch 1 Batch_Num 43 Loss 0.01835029572248459\n",
      "Epoch 1 Batch_Num 44 Loss 0.02747192420065403\n",
      "Epoch 1 Batch_Num 45 Loss 0.04104377329349518\n",
      "Epoch 1 Batch_Num 46 Loss 0.032471176236867905\n",
      "Epoch 1 Batch_Num 47 Loss 0.015160569921135902\n",
      "Epoch 1 Batch_Num 48 Loss 0.013253668323159218\n",
      "Epoch 1 Batch_Num 49 Loss 0.025363612920045853\n",
      "Epoch 1 Batch_Num 50 Loss 0.02072214148938656\n",
      "Epoch 1 Batch_Num 51 Loss 0.04114041477441788\n",
      "Epoch 1 Batch_Num 52 Loss 0.02146398089826107\n",
      "Epoch 1 Batch_Num 53 Loss 0.018010037019848824\n",
      "Epoch 1 Batch_Num 54 Loss 0.02172701619565487\n",
      "Epoch 1 Batch_Num 55 Loss 0.019033849239349365\n",
      "Epoch 1 Batch_Num 56 Loss 0.020846685394644737\n",
      "Epoch 1 Batch_Num 57 Loss 0.022268442437052727\n",
      "Epoch 1 Batch_Num 58 Loss 0.010015787556767464\n",
      "Epoch 1 Batch_Num 59 Loss 0.02010929398238659\n",
      "Epoch 1 Batch_Num 60 Loss 0.01709689199924469\n",
      "Epoch 1 Batch_Num 61 Loss 0.027301928028464317\n",
      "Epoch 1 Batch_Num 62 Loss 0.025053007528185844\n",
      "Epoch 1 Batch_Num 63 Loss 0.010639580897986889\n",
      "Epoch 1 Batch_Num 64 Loss 0.0138871343806386\n",
      "Epoch 1 Batch_Num 65 Loss 0.022059034556150436\n",
      "Epoch 1 Batch_Num 66 Loss 0.023783398792147636\n",
      "Epoch 1 Batch_Num 67 Loss 0.017592305317521095\n",
      "Epoch 1 Batch_Num 68 Loss 0.029695605859160423\n",
      "Epoch 1 Batch_Num 69 Loss 0.020876243710517883\n",
      "Epoch 1 Batch_Num 70 Loss 0.0138264624401927\n",
      "Epoch 1 Batch_Num 71 Loss 0.014031055383384228\n",
      "Epoch 1 Batch_Num 72 Loss 0.012671281583607197\n",
      "Epoch 1 Batch_Num 73 Loss 0.006840156856924295\n",
      "Epoch 1 Batch_Num 74 Loss 0.021209025755524635\n",
      "Epoch 1 Batch_Num 75 Loss 0.024193445220589638\n",
      "Epoch 1 Batch_Num 76 Loss 0.021660035476088524\n",
      "Epoch 1 Batch_Num 77 Loss 0.016717951744794846\n",
      "Epoch 1 Batch_Num 78 Loss 0.029618067666888237\n",
      "Epoch 1 Batch_Num 79 Loss 0.013668932020664215\n",
      "Epoch 1 Batch_Num 80 Loss 0.0217678751796484\n",
      "Epoch 1 Batch_Num 81 Loss 0.01855720393359661\n",
      "Epoch 1 Batch_Num 82 Loss 0.016450611874461174\n",
      "Epoch 1 Batch_Num 83 Loss 0.0157235786318779\n",
      "Epoch 1 Batch_Num 84 Loss 0.015371227636933327\n",
      "Epoch 1 Batch_Num 85 Loss 0.013489052653312683\n",
      "Epoch 1 Batch_Num 86 Loss 0.01347604300826788\n",
      "Epoch 1 Batch_Num 87 Loss 0.012415456585586071\n",
      "Epoch 1 Batch_Num 88 Loss 0.014420181512832642\n",
      "Epoch 1 Batch_Num 89 Loss 0.004838456865400076\n",
      "Epoch 1 Batch_Num 90 Loss 0.01802041195333004\n",
      "Epoch 1 Batch_Num 91 Loss 0.01625724509358406\n",
      "Epoch 1 Batch_Num 92 Loss 0.02117861434817314\n",
      "Epoch 1 Batch_Num 93 Loss 0.019748041406273842\n",
      "Epoch 1 Batch_Num 94 Loss 0.009347140789031982\n",
      "Epoch 1 Batch_Num 95 Loss 0.01158268004655838\n",
      "Epoch 1 Batch_Num 96 Loss 0.029894782230257988\n",
      "Epoch 1 Batch_Num 97 Loss 0.0169599000364542\n",
      "Epoch 1 Batch_Num 98 Loss 0.015773717314004898\n",
      "Epoch 1 Batch_Num 99 Loss 0.01227326225489378\n",
      "Epoch 1 Batch_Num 100 Loss 0.024382783100008965\n",
      "Epoch 1 Batch_Num 101 Loss 0.015209050849080086\n",
      "Epoch 1 Batch_Num 102 Loss 0.009959098882973194\n",
      "Epoch 1 Batch_Num 103 Loss 0.03487853333353996\n",
      "Epoch 1 Batch_Num 104 Loss 0.031039340421557426\n",
      "Epoch 1 Batch_Num 105 Loss 0.01897127740085125\n",
      "Epoch 1 Batch_Num 106 Loss 0.015204355120658875\n",
      "Epoch 1 Batch_Num 107 Loss 0.024338236078619957\n",
      "Epoch 1 Batch_Num 108 Loss 0.014850052073597908\n",
      "Epoch 1 Batch_Num 109 Loss 0.011391964741051197\n",
      "Epoch 1 Batch_Num 110 Loss 0.01949840597808361\n",
      "Epoch 1 Batch_Num 111 Loss 0.026018580421805382\n",
      "Epoch 1 Batch_Num 112 Loss 0.015189087949693203\n",
      "Epoch 1 Batch_Num 113 Loss 0.018796266987919807\n",
      "Epoch 1 Batch_Num 114 Loss 0.017020821571350098\n",
      "Epoch 1 Batch_Num 115 Loss 0.016680501401424408\n",
      "Epoch 1 Batch_Num 116 Loss 0.017139816656708717\n",
      "Epoch 1 Batch_Num 117 Loss 0.019843928515911102\n",
      "Epoch 1 Batch_Num 118 Loss 0.006581801921129227\n",
      "Epoch 1 Batch_Num 119 Loss 0.022764069959521294\n",
      "Epoch 1 Batch_Num 120 Loss 0.013822649605572224\n",
      "Epoch 1 Batch_Num 121 Loss 0.01578652486205101\n",
      "Epoch 1 Batch_Num 122 Loss 0.02078596130013466\n",
      "Epoch 1 Batch_Num 123 Loss 0.014709812588989735\n",
      "Epoch 1 Batch_Num 124 Loss 0.01886434108018875\n",
      "Epoch 1 Batch_Num 125 Loss 0.01435148622840643\n",
      "Epoch 1 Batch_Num 126 Loss 0.02230103872716427\n",
      "Epoch 1 Batch_Num 127 Loss 0.008709678426384926\n",
      "Epoch 1 Batch_Num 128 Loss 0.014317630790174007\n",
      "Epoch 1 Batch_Num 129 Loss 0.012129890732467175\n",
      "Epoch 1 Batch_Num 130 Loss 0.019763072952628136\n",
      "Epoch 1 Batch_Num 131 Loss 0.01730450987815857\n",
      "Epoch 1 Batch_Num 132 Loss 0.005760289262980223\n",
      "Epoch 1 Batch_Num 133 Loss 0.0076814256608486176\n",
      "Epoch 1 Batch_Num 134 Loss 0.026133108884096146\n",
      "Epoch 1 Batch_Num 135 Loss 0.021769318729639053\n",
      "Epoch 1 Batch_Num 136 Loss 0.014040203765034676\n",
      "Epoch 1 Batch_Num 137 Loss 0.01696135848760605\n",
      "Epoch 1 Batch_Num 138 Loss 0.009036672301590443\n",
      "Epoch 1 Batch_Num 139 Loss 0.014398290775716305\n",
      "Epoch 1 Batch_Num 140 Loss 0.015597301535308361\n",
      "Epoch 1 Batch_Num 141 Loss 0.016657963395118713\n",
      "Epoch 1 Batch_Num 142 Loss 0.01956329680979252\n",
      "Epoch 1 Batch_Num 143 Loss 0.018834127113223076\n",
      "Epoch 1 Batch_Num 144 Loss 0.01200694777071476\n",
      "Epoch 1 Batch_Num 145 Loss 0.009191048331558704\n",
      "Epoch 1 Batch_Num 146 Loss 0.025086045265197754\n",
      "Epoch 1 Batch_Num 147 Loss 0.021975738927721977\n",
      "Epoch 1 Batch_Num 148 Loss 0.008223118260502815\n",
      "Epoch 1 Batch_Num 149 Loss 0.01516107190400362\n",
      "Epoch 1 Batch_Num 150 Loss 0.011078198440372944\n",
      "Epoch 1 Batch_Num 151 Loss 0.011976523324847221\n",
      "Epoch 1 Batch_Num 152 Loss 0.0061483196914196014\n",
      "Epoch 1 Batch_Num 153 Loss 0.03694617748260498\n",
      "Epoch 1 Batch_Num 154 Loss 0.01709749735891819\n",
      "Epoch 1 Batch_Num 155 Loss 0.016658129170536995\n",
      "Epoch 1 Batch_Num 156 Loss 0.010526689700782299\n",
      "Epoch 1 Batch_Num 157 Loss 0.015164962969720364\n",
      "Epoch 1 Batch_Num 158 Loss 0.008811297826468945\n",
      "Epoch 1 Batch_Num 159 Loss 0.009789700619876385\n",
      "Epoch 1 Batch_Num 160 Loss 0.012422832660377026\n",
      "Epoch 1 Batch_Num 161 Loss 0.02066432312130928\n",
      "Epoch 1 Batch_Num 162 Loss 0.012331528589129448\n",
      "Epoch 1 Batch_Num 163 Loss 0.01435312069952488\n",
      "Epoch 1 Batch_Num 164 Loss 0.015611434355378151\n",
      "Epoch 1 Batch_Num 165 Loss 0.01578475721180439\n",
      "Epoch 1 Batch_Num 166 Loss 0.016082074493169785\n",
      "Epoch 1 Batch_Num 167 Loss 0.022335700690746307\n",
      "Epoch 1 Batch_Num 168 Loss 0.017728706821799278\n",
      "Epoch 1 Batch_Num 169 Loss 0.012195853516459465\n",
      "Epoch 1 Batch_Num 170 Loss 0.013868271373212337\n",
      "Epoch 1 Batch_Num 171 Loss 0.01725667156279087\n",
      "Epoch 1 Batch_Num 172 Loss 0.02200324647128582\n",
      "Epoch 1 Batch_Num 173 Loss 0.022587459534406662\n",
      "Epoch 1 Batch_Num 174 Loss 0.006096700672060251\n",
      "Epoch 1 Batch_Num 175 Loss 0.01635967753827572\n",
      "Epoch 1 Batch_Num 176 Loss 0.010209109634160995\n",
      "Epoch 1 Batch_Num 177 Loss 0.01590629294514656\n",
      "Epoch 1 Batch_Num 178 Loss 0.015158665366470814\n",
      "Epoch 1 Batch_Num 179 Loss 0.019543295726180077\n",
      "Epoch 1 Batch_Num 180 Loss 0.018108082935214043\n",
      "Epoch 1 Batch_Num 181 Loss 0.011128357611596584\n",
      "Epoch 1 Batch_Num 182 Loss 0.011329560540616512\n",
      "Epoch 1 Batch_Num 183 Loss 0.021625179797410965\n",
      "Epoch 1 Batch_Num 184 Loss 0.023374924436211586\n",
      "Epoch 1 Batch_Num 185 Loss 0.025244200602173805\n",
      "Epoch 1 Batch_Num 186 Loss 0.011940510012209415\n",
      "Epoch 1 Batch_Num 187 Loss 0.011420711874961853\n",
      "Epoch 1 Batch_Num 188 Loss 0.015453914180397987\n",
      "Epoch 1 Batch_Num 189 Loss 0.008791796863079071\n",
      "Epoch 1 Batch_Num 190 Loss 0.006046799477189779\n",
      "Epoch 1 Batch_Num 191 Loss 0.030088508501648903\n",
      "Epoch 1 Batch_Num 192 Loss 0.010468652471899986\n",
      "Epoch 1 Batch_Num 193 Loss 0.02099607326090336\n",
      "Epoch 1 Batch_Num 194 Loss 0.018695278093218803\n",
      "Epoch 1 Batch_Num 195 Loss 0.010044164024293423\n",
      "Epoch 1 Batch_Num 196 Loss 0.018642056733369827\n",
      "Epoch 1 Batch_Num 197 Loss 0.010781770572066307\n",
      "Epoch 1 Batch_Num 198 Loss 0.01764361932873726\n",
      "Epoch 1 Batch_Num 199 Loss 0.0068610492162406445\n",
      "Epoch 1 Batch_Num 200 Loss 0.01476281601935625\n",
      "Epoch 1 Batch_Num 201 Loss 0.010495880618691444\n",
      "Epoch 1 Batch_Num 202 Loss 0.010327699594199657\n",
      "Epoch 1 Batch_Num 203 Loss 0.03008713759481907\n",
      "Epoch 1 Batch_Num 204 Loss 0.01782054640352726\n",
      "Epoch 1 Batch_Num 205 Loss 0.010433760471642017\n",
      "Epoch 1 Batch_Num 206 Loss 0.03763818368315697\n",
      "Epoch 1 Batch_Num 207 Loss 0.011125404387712479\n",
      "Epoch 1 Batch_Num 208 Loss 0.00712056178599596\n",
      "Epoch 1 Batch_Num 209 Loss 0.02051493152976036\n",
      "Epoch 1 Batch_Num 210 Loss 0.026424584910273552\n",
      "Epoch 1 Batch_Num 211 Loss 0.01712268777191639\n",
      "Epoch 1 Batch_Num 212 Loss 0.008480916731059551\n",
      "Epoch 1 Batch_Num 213 Loss 0.007971730083227158\n",
      "Epoch 1 Batch_Num 214 Loss 0.012739687226712704\n",
      "Epoch 1 Batch_Num 215 Loss 0.010909500531852245\n",
      "Epoch 1 Batch_Num 216 Loss 0.018885454162955284\n",
      "Epoch 1 Batch_Num 217 Loss 0.016348158940672874\n",
      "Epoch 1 Batch_Num 218 Loss 0.014242020435631275\n",
      "Epoch 1 Batch_Num 219 Loss 0.011281977407634258\n",
      "Epoch 1 Batch_Num 220 Loss 0.01288487296551466\n",
      "Epoch 1 Batch_Num 221 Loss 0.016110768541693687\n",
      "Epoch 1 Batch_Num 222 Loss 0.011149775236845016\n",
      "Epoch 1 Batch_Num 223 Loss 0.010631980374455452\n",
      "Epoch 1 Batch_Num 224 Loss 0.0125541677698493\n",
      "Epoch 1 Batch_Num 225 Loss 0.019758129492402077\n",
      "Epoch 1 Batch_Num 226 Loss 0.013743913732469082\n",
      "Epoch 1 Batch_Num 227 Loss 0.0013714049709960818\n",
      "Epoch 1 Batch_Num 228 Loss 0.0033548970241099596\n",
      "Epoch 1 Batch_Num 229 Loss 0.008820213377475739\n",
      "Epoch 1 Batch_Num 230 Loss 0.00045560338185168803\n",
      "Epoch 1 Batch_Num 231 Loss 0.007434332277625799\n",
      "Epoch 1 Batch_Num 232 Loss 0.011743806302547455\n",
      "Epoch 1 Batch_Num 233 Loss 0.02122027613222599\n",
      "Epoch 1 Batch_Num 234 Loss 0.02570892684161663\n",
      "Epoch 2 Batch_Num 0 Loss 0.01260219793766737\n",
      "Epoch 2 Batch_Num 1 Loss 0.017128465697169304\n",
      "Epoch 2 Batch_Num 2 Loss 0.012602433562278748\n",
      "Epoch 2 Batch_Num 3 Loss 0.022749660536646843\n",
      "Epoch 2 Batch_Num 4 Loss 0.02027646265923977\n",
      "Epoch 2 Batch_Num 5 Loss 0.014545365236699581\n",
      "Epoch 2 Batch_Num 6 Loss 0.014322531409561634\n",
      "Epoch 2 Batch_Num 7 Loss 0.005317355040460825\n",
      "Epoch 2 Batch_Num 8 Loss 0.0065067424438893795\n",
      "Epoch 2 Batch_Num 9 Loss 0.005787175614386797\n",
      "Epoch 2 Batch_Num 10 Loss 0.016115564852952957\n",
      "Epoch 2 Batch_Num 11 Loss 0.011048082262277603\n",
      "Epoch 2 Batch_Num 12 Loss 0.009044907987117767\n",
      "Epoch 2 Batch_Num 13 Loss 0.014425361528992653\n",
      "Epoch 2 Batch_Num 14 Loss 0.013317828066647053\n",
      "Epoch 2 Batch_Num 15 Loss 0.004444952588528395\n",
      "Epoch 2 Batch_Num 16 Loss 0.006562358234077692\n",
      "Epoch 2 Batch_Num 17 Loss 0.012271721847355366\n",
      "Epoch 2 Batch_Num 18 Loss 0.00876187440007925\n",
      "Epoch 2 Batch_Num 19 Loss 0.020964331924915314\n",
      "Epoch 2 Batch_Num 20 Loss 0.01837213896214962\n",
      "Epoch 2 Batch_Num 21 Loss 0.004519728943705559\n",
      "Epoch 2 Batch_Num 22 Loss 0.017720285803079605\n",
      "Epoch 2 Batch_Num 23 Loss 0.006872219033539295\n",
      "Epoch 2 Batch_Num 24 Loss 0.012890986166894436\n",
      "Epoch 2 Batch_Num 25 Loss 0.008792633190751076\n",
      "Epoch 2 Batch_Num 26 Loss 0.014990612864494324\n",
      "Epoch 2 Batch_Num 27 Loss 0.016600580886006355\n",
      "Epoch 2 Batch_Num 28 Loss 0.017236405983567238\n",
      "Epoch 2 Batch_Num 29 Loss 0.006776162888854742\n",
      "Epoch 2 Batch_Num 30 Loss 0.011257235892117023\n",
      "Epoch 2 Batch_Num 31 Loss 0.00980033166706562\n",
      "Epoch 2 Batch_Num 32 Loss 0.017180709168314934\n",
      "Epoch 2 Batch_Num 33 Loss 0.00784959178417921\n",
      "Epoch 2 Batch_Num 34 Loss 0.01855403743684292\n",
      "Epoch 2 Batch_Num 35 Loss 0.0076951864175498486\n",
      "Epoch 2 Batch_Num 36 Loss 0.01735665462911129\n",
      "Epoch 2 Batch_Num 37 Loss 0.008200292475521564\n",
      "Epoch 2 Batch_Num 38 Loss 0.007987181656062603\n",
      "Epoch 2 Batch_Num 39 Loss 0.011273554526269436\n",
      "Epoch 2 Batch_Num 40 Loss 0.021253127604722977\n",
      "Epoch 2 Batch_Num 41 Loss 0.004461995791643858\n",
      "Epoch 2 Batch_Num 42 Loss 0.012800483964383602\n",
      "Epoch 2 Batch_Num 43 Loss 0.009926047176122665\n",
      "Epoch 2 Batch_Num 44 Loss 0.010539853945374489\n",
      "Epoch 2 Batch_Num 45 Loss 0.02272261120378971\n",
      "Epoch 2 Batch_Num 46 Loss 0.02745649218559265\n",
      "Epoch 2 Batch_Num 47 Loss 0.013093657791614532\n",
      "Epoch 2 Batch_Num 48 Loss 0.010174478404223919\n",
      "Epoch 2 Batch_Num 49 Loss 0.015544235706329346\n",
      "Epoch 2 Batch_Num 50 Loss 0.011280483566224575\n",
      "Epoch 2 Batch_Num 51 Loss 0.023279404267668724\n",
      "Epoch 2 Batch_Num 52 Loss 0.016054101288318634\n",
      "Epoch 2 Batch_Num 53 Loss 0.015559843741357327\n",
      "Epoch 2 Batch_Num 54 Loss 0.014910203404724598\n",
      "Epoch 2 Batch_Num 55 Loss 0.013085897080600262\n",
      "Epoch 2 Batch_Num 56 Loss 0.0176310446113348\n",
      "Epoch 2 Batch_Num 57 Loss 0.009424715302884579\n",
      "Epoch 2 Batch_Num 58 Loss 0.007038932293653488\n",
      "Epoch 2 Batch_Num 59 Loss 0.014175555668771267\n",
      "Epoch 2 Batch_Num 60 Loss 0.011995992623269558\n",
      "Epoch 2 Batch_Num 61 Loss 0.013718283735215664\n",
      "Epoch 2 Batch_Num 62 Loss 0.015207785181701183\n",
      "Epoch 2 Batch_Num 63 Loss 0.012818211689591408\n",
      "Epoch 2 Batch_Num 64 Loss 0.014896835200488567\n",
      "Epoch 2 Batch_Num 65 Loss 0.015477052889764309\n",
      "Epoch 2 Batch_Num 66 Loss 0.014588544145226479\n",
      "Epoch 2 Batch_Num 67 Loss 0.014544382691383362\n",
      "Epoch 2 Batch_Num 68 Loss 0.015353396534919739\n",
      "Epoch 2 Batch_Num 69 Loss 0.013449611142277718\n",
      "Epoch 2 Batch_Num 70 Loss 0.012164080515503883\n",
      "Epoch 2 Batch_Num 71 Loss 0.011065059341490269\n",
      "Epoch 2 Batch_Num 72 Loss 0.010357675142586231\n",
      "Epoch 2 Batch_Num 73 Loss 0.005524844862520695\n",
      "Epoch 2 Batch_Num 74 Loss 0.013382362201809883\n",
      "Epoch 2 Batch_Num 75 Loss 0.014591648243367672\n",
      "Epoch 2 Batch_Num 76 Loss 0.006432980298995972\n",
      "Epoch 2 Batch_Num 77 Loss 0.01062824297696352\n",
      "Epoch 2 Batch_Num 78 Loss 0.023419244214892387\n",
      "Epoch 2 Batch_Num 79 Loss 0.010678926482796669\n",
      "Epoch 2 Batch_Num 80 Loss 0.020642628893256187\n",
      "Epoch 2 Batch_Num 81 Loss 0.015526004135608673\n",
      "Epoch 2 Batch_Num 82 Loss 0.011995564214885235\n",
      "Epoch 2 Batch_Num 83 Loss 0.004669529851526022\n",
      "Epoch 2 Batch_Num 84 Loss 0.01617990806698799\n",
      "Epoch 2 Batch_Num 85 Loss 0.010259930975735188\n",
      "Epoch 2 Batch_Num 86 Loss 0.00999230332672596\n",
      "Epoch 2 Batch_Num 87 Loss 0.010988465510308743\n",
      "Epoch 2 Batch_Num 88 Loss 0.01489847619086504\n",
      "Epoch 2 Batch_Num 89 Loss 0.001786420471034944\n",
      "Epoch 2 Batch_Num 90 Loss 0.01552001666277647\n",
      "Epoch 2 Batch_Num 91 Loss 0.009208721108734608\n",
      "Epoch 2 Batch_Num 92 Loss 0.028371436521410942\n",
      "Epoch 2 Batch_Num 93 Loss 0.01249608863145113\n",
      "Epoch 2 Batch_Num 94 Loss 0.007896746508777142\n",
      "Epoch 2 Batch_Num 95 Loss 0.00857942271977663\n",
      "Epoch 2 Batch_Num 96 Loss 0.01768311858177185\n",
      "Epoch 2 Batch_Num 97 Loss 0.00981821771711111\n",
      "Epoch 2 Batch_Num 98 Loss 0.011906987056136131\n",
      "Epoch 2 Batch_Num 99 Loss 0.009217779152095318\n",
      "Epoch 2 Batch_Num 100 Loss 0.009741000831127167\n",
      "Epoch 2 Batch_Num 101 Loss 0.009204569272696972\n",
      "Epoch 2 Batch_Num 102 Loss 0.005097970366477966\n",
      "Epoch 2 Batch_Num 103 Loss 0.023578427731990814\n",
      "Epoch 2 Batch_Num 104 Loss 0.021307818591594696\n",
      "Epoch 2 Batch_Num 105 Loss 0.012627449817955494\n",
      "Epoch 2 Batch_Num 106 Loss 0.011250611394643784\n",
      "Epoch 2 Batch_Num 107 Loss 0.016881059855222702\n",
      "Epoch 2 Batch_Num 108 Loss 0.017889006063342094\n",
      "Epoch 2 Batch_Num 109 Loss 0.010052132420241833\n",
      "Epoch 2 Batch_Num 110 Loss 0.011353696696460247\n",
      "Epoch 2 Batch_Num 111 Loss 0.017842864617705345\n",
      "Epoch 2 Batch_Num 112 Loss 0.008065340109169483\n",
      "Epoch 2 Batch_Num 113 Loss 0.01017056591808796\n",
      "Epoch 2 Batch_Num 114 Loss 0.014712703414261341\n",
      "Epoch 2 Batch_Num 115 Loss 0.012899093329906464\n",
      "Epoch 2 Batch_Num 116 Loss 0.01461311150342226\n",
      "Epoch 2 Batch_Num 117 Loss 0.013625136576592922\n",
      "Epoch 2 Batch_Num 118 Loss 0.005572716239839792\n",
      "Epoch 2 Batch_Num 119 Loss 0.014393600635230541\n",
      "Epoch 2 Batch_Num 120 Loss 0.010308575816452503\n",
      "Epoch 2 Batch_Num 121 Loss 0.012370861135423183\n",
      "Epoch 2 Batch_Num 122 Loss 0.013419619761407375\n",
      "Epoch 2 Batch_Num 123 Loss 0.010698034428060055\n",
      "Epoch 2 Batch_Num 124 Loss 0.015021075494587421\n",
      "Epoch 2 Batch_Num 125 Loss 0.008180282078683376\n",
      "Epoch 2 Batch_Num 126 Loss 0.019200069829821587\n",
      "Epoch 2 Batch_Num 127 Loss 0.0034680613316595554\n",
      "Epoch 2 Batch_Num 128 Loss 0.010557360015809536\n",
      "Epoch 2 Batch_Num 129 Loss 0.009224792011082172\n",
      "Epoch 2 Batch_Num 130 Loss 0.01586712710559368\n",
      "Epoch 2 Batch_Num 131 Loss 0.011668195016682148\n",
      "Epoch 2 Batch_Num 132 Loss 0.0035657640546560287\n",
      "Epoch 2 Batch_Num 133 Loss 0.004183530807495117\n",
      "Epoch 2 Batch_Num 134 Loss 0.016841648146510124\n",
      "Epoch 2 Batch_Num 135 Loss 0.0159575417637825\n",
      "Epoch 2 Batch_Num 136 Loss 0.010061688721179962\n",
      "Epoch 2 Batch_Num 137 Loss 0.015175220556557178\n",
      "Epoch 2 Batch_Num 138 Loss 0.006221456453204155\n",
      "Epoch 2 Batch_Num 139 Loss 0.01188815850764513\n",
      "Epoch 2 Batch_Num 140 Loss 0.011401447467505932\n",
      "Epoch 2 Batch_Num 141 Loss 0.008870777674019337\n",
      "Epoch 2 Batch_Num 142 Loss 0.013296033255755901\n",
      "Epoch 2 Batch_Num 143 Loss 0.010541490279138088\n",
      "Epoch 2 Batch_Num 144 Loss 0.00918913260102272\n",
      "Epoch 2 Batch_Num 145 Loss 0.007205152418464422\n",
      "Epoch 2 Batch_Num 146 Loss 0.01367814838886261\n",
      "Epoch 2 Batch_Num 147 Loss 0.017406394705176353\n",
      "Epoch 2 Batch_Num 148 Loss 0.005554355680942535\n",
      "Epoch 2 Batch_Num 149 Loss 0.012632747180759907\n",
      "Epoch 2 Batch_Num 150 Loss 0.00907888077199459\n",
      "Epoch 2 Batch_Num 151 Loss 0.006587793119251728\n",
      "Epoch 2 Batch_Num 152 Loss 0.005084056872874498\n",
      "Epoch 2 Batch_Num 153 Loss 0.02614227868616581\n",
      "Epoch 2 Batch_Num 154 Loss 0.010970399715006351\n",
      "Epoch 2 Batch_Num 155 Loss 0.011827269569039345\n",
      "Epoch 2 Batch_Num 156 Loss 0.007758516818284988\n",
      "Epoch 2 Batch_Num 157 Loss 0.01193916704505682\n",
      "Epoch 2 Batch_Num 158 Loss 0.006942283362150192\n",
      "Epoch 2 Batch_Num 159 Loss 0.0037642510142177343\n",
      "Epoch 2 Batch_Num 160 Loss 0.008421373553574085\n",
      "Epoch 2 Batch_Num 161 Loss 0.018989823758602142\n",
      "Epoch 2 Batch_Num 162 Loss 0.008088129572570324\n",
      "Epoch 2 Batch_Num 163 Loss 0.008355632424354553\n",
      "Epoch 2 Batch_Num 164 Loss 0.008925083093345165\n",
      "Epoch 2 Batch_Num 165 Loss 0.00828543584793806\n",
      "Epoch 2 Batch_Num 166 Loss 0.015480262227356434\n",
      "Epoch 2 Batch_Num 167 Loss 0.012882052920758724\n",
      "Epoch 2 Batch_Num 168 Loss 0.010919580236077309\n",
      "Epoch 2 Batch_Num 169 Loss 0.0073439329862594604\n",
      "Epoch 2 Batch_Num 170 Loss 0.011535855941474438\n",
      "Epoch 2 Batch_Num 171 Loss 0.010765159502625465\n",
      "Epoch 2 Batch_Num 172 Loss 0.014328353106975555\n",
      "Epoch 2 Batch_Num 173 Loss 0.01402806956321001\n",
      "Epoch 2 Batch_Num 174 Loss 0.004275925923138857\n",
      "Epoch 2 Batch_Num 175 Loss 0.010167309083044529\n",
      "Epoch 2 Batch_Num 176 Loss 0.00817453395575285\n",
      "Epoch 2 Batch_Num 177 Loss 0.011868241243064404\n",
      "Epoch 2 Batch_Num 178 Loss 0.009642750024795532\n",
      "Epoch 2 Batch_Num 179 Loss 0.015991464257240295\n",
      "Epoch 2 Batch_Num 180 Loss 0.011137600056827068\n",
      "Epoch 2 Batch_Num 181 Loss 0.010667882859706879\n",
      "Epoch 2 Batch_Num 182 Loss 0.004499772097915411\n",
      "Epoch 2 Batch_Num 183 Loss 0.012731817550957203\n",
      "Epoch 2 Batch_Num 184 Loss 0.013593629002571106\n",
      "Epoch 2 Batch_Num 185 Loss 0.016453707590699196\n",
      "Epoch 2 Batch_Num 186 Loss 0.009934498928487301\n",
      "Epoch 2 Batch_Num 187 Loss 0.010443799197673798\n",
      "Epoch 2 Batch_Num 188 Loss 0.009789763949811459\n",
      "Epoch 2 Batch_Num 189 Loss 0.005519128870218992\n",
      "Epoch 2 Batch_Num 190 Loss 0.0026269357185810804\n",
      "Epoch 2 Batch_Num 191 Loss 0.025487661361694336\n",
      "Epoch 2 Batch_Num 192 Loss 0.014936971478164196\n",
      "Epoch 2 Batch_Num 193 Loss 0.026868296787142754\n",
      "Epoch 2 Batch_Num 194 Loss 0.012097504921257496\n",
      "Epoch 2 Batch_Num 195 Loss 0.005760413594543934\n",
      "Epoch 2 Batch_Num 196 Loss 0.014361334033310413\n",
      "Epoch 2 Batch_Num 197 Loss 0.008997703902423382\n",
      "Epoch 2 Batch_Num 198 Loss 0.021948879584670067\n",
      "Epoch 2 Batch_Num 199 Loss 0.006668682675808668\n",
      "Epoch 2 Batch_Num 200 Loss 0.010504232719540596\n",
      "Epoch 2 Batch_Num 201 Loss 0.006845332216471434\n",
      "Epoch 2 Batch_Num 202 Loss 0.0052415840327739716\n",
      "Epoch 2 Batch_Num 203 Loss 0.02111089788377285\n",
      "Epoch 2 Batch_Num 204 Loss 0.02182834967970848\n",
      "Epoch 2 Batch_Num 205 Loss 0.013001001439988613\n",
      "Epoch 2 Batch_Num 206 Loss 0.027675701305270195\n",
      "Epoch 2 Batch_Num 207 Loss 0.00950045045465231\n",
      "Epoch 2 Batch_Num 208 Loss 0.006245618220418692\n",
      "Epoch 2 Batch_Num 209 Loss 0.013733579777181149\n",
      "Epoch 2 Batch_Num 210 Loss 0.02354043535888195\n",
      "Epoch 2 Batch_Num 211 Loss 0.019326049834489822\n",
      "Epoch 2 Batch_Num 212 Loss 0.007004093378782272\n",
      "Epoch 2 Batch_Num 213 Loss 0.010806922800838947\n",
      "Epoch 2 Batch_Num 214 Loss 0.008531717583537102\n",
      "Epoch 2 Batch_Num 215 Loss 0.004310033284127712\n",
      "Epoch 2 Batch_Num 216 Loss 0.011256079189479351\n",
      "Epoch 2 Batch_Num 217 Loss 0.012686001136898994\n",
      "Epoch 2 Batch_Num 218 Loss 0.014706897549331188\n",
      "Epoch 2 Batch_Num 219 Loss 0.01005010586231947\n",
      "Epoch 2 Batch_Num 220 Loss 0.0077443718910217285\n",
      "Epoch 2 Batch_Num 221 Loss 0.015170032158493996\n",
      "Epoch 2 Batch_Num 222 Loss 0.013000178150832653\n",
      "Epoch 2 Batch_Num 223 Loss 0.009552998468279839\n",
      "Epoch 2 Batch_Num 224 Loss 0.007260553538799286\n",
      "Epoch 2 Batch_Num 225 Loss 0.013456612825393677\n",
      "Epoch 2 Batch_Num 226 Loss 0.009288943372666836\n",
      "Epoch 2 Batch_Num 227 Loss 0.0007047633989714086\n",
      "Epoch 2 Batch_Num 228 Loss 0.0016232014168053865\n",
      "Epoch 2 Batch_Num 229 Loss 0.0035420625936239958\n",
      "Epoch 2 Batch_Num 230 Loss 0.0005928202881477773\n",
      "Epoch 2 Batch_Num 231 Loss 0.00535476254299283\n",
      "Epoch 2 Batch_Num 232 Loss 0.005994971375912428\n",
      "Epoch 2 Batch_Num 233 Loss 0.016088664531707764\n",
      "Epoch 2 Batch_Num 234 Loss 0.019595814868807793\n",
      "Epoch 3 Batch_Num 0 Loss 0.012265587225556374\n",
      "Epoch 3 Batch_Num 1 Loss 0.0128429951146245\n",
      "Epoch 3 Batch_Num 2 Loss 0.010380568914115429\n",
      "Epoch 3 Batch_Num 3 Loss 0.01642727665603161\n",
      "Epoch 3 Batch_Num 4 Loss 0.010791431181132793\n",
      "Epoch 3 Batch_Num 5 Loss 0.007852631621062756\n",
      "Epoch 3 Batch_Num 6 Loss 0.013017773628234863\n",
      "Epoch 3 Batch_Num 7 Loss 0.006944103632122278\n",
      "Epoch 3 Batch_Num 8 Loss 0.003719619708135724\n",
      "Epoch 3 Batch_Num 9 Loss 0.007015471812337637\n",
      "Epoch 3 Batch_Num 10 Loss 0.012527929618954659\n",
      "Epoch 3 Batch_Num 11 Loss 0.009717525914311409\n",
      "Epoch 3 Batch_Num 12 Loss 0.004039701074361801\n",
      "Epoch 3 Batch_Num 13 Loss 0.008298704400658607\n",
      "Epoch 3 Batch_Num 14 Loss 0.01362824160605669\n",
      "Epoch 3 Batch_Num 15 Loss 0.0037547163665294647\n",
      "Epoch 3 Batch_Num 16 Loss 0.004662186838686466\n",
      "Epoch 3 Batch_Num 17 Loss 0.008772407658398151\n",
      "Epoch 3 Batch_Num 18 Loss 0.005417475011199713\n",
      "Epoch 3 Batch_Num 19 Loss 0.015023186802864075\n",
      "Epoch 3 Batch_Num 20 Loss 0.014321453869342804\n",
      "Epoch 3 Batch_Num 21 Loss 0.003275708993896842\n",
      "Epoch 3 Batch_Num 22 Loss 0.016663022339344025\n",
      "Epoch 3 Batch_Num 23 Loss 0.005353301763534546\n",
      "Epoch 3 Batch_Num 24 Loss 0.008180856704711914\n",
      "Epoch 3 Batch_Num 25 Loss 0.00562513479962945\n",
      "Epoch 3 Batch_Num 26 Loss 0.013778135180473328\n",
      "Epoch 3 Batch_Num 27 Loss 0.015203463844954967\n",
      "Epoch 3 Batch_Num 28 Loss 0.012109576724469662\n",
      "Epoch 3 Batch_Num 29 Loss 0.006556913256645203\n",
      "Epoch 3 Batch_Num 30 Loss 0.009961139410734177\n",
      "Epoch 3 Batch_Num 31 Loss 0.01035889983177185\n",
      "Epoch 3 Batch_Num 32 Loss 0.01521080918610096\n",
      "Epoch 3 Batch_Num 33 Loss 0.006527912802994251\n",
      "Epoch 3 Batch_Num 34 Loss 0.0129736652597785\n",
      "Epoch 3 Batch_Num 35 Loss 0.00869803223758936\n",
      "Epoch 3 Batch_Num 36 Loss 0.018666798248887062\n",
      "Epoch 3 Batch_Num 37 Loss 0.006077922414988279\n",
      "Epoch 3 Batch_Num 38 Loss 0.008068463765084743\n",
      "Epoch 3 Batch_Num 39 Loss 0.0063546872697770596\n",
      "Epoch 3 Batch_Num 40 Loss 0.020100129768252373\n",
      "Epoch 3 Batch_Num 41 Loss 0.0033073201775550842\n",
      "Epoch 3 Batch_Num 42 Loss 0.01217206846922636\n",
      "Epoch 3 Batch_Num 43 Loss 0.008056573569774628\n",
      "Epoch 3 Batch_Num 44 Loss 0.0041100517846643925\n",
      "Epoch 3 Batch_Num 45 Loss 0.014020624570548534\n",
      "Epoch 3 Batch_Num 46 Loss 0.020802034065127373\n",
      "Epoch 3 Batch_Num 47 Loss 0.007330724503844976\n",
      "Epoch 3 Batch_Num 48 Loss 0.008499679155647755\n",
      "Epoch 3 Batch_Num 49 Loss 0.011549103073775768\n",
      "Epoch 3 Batch_Num 50 Loss 0.013996235094964504\n",
      "Epoch 3 Batch_Num 51 Loss 0.016957001760601997\n",
      "Epoch 3 Batch_Num 52 Loss 0.012169833295047283\n",
      "Epoch 3 Batch_Num 53 Loss 0.014687825925648212\n",
      "Epoch 3 Batch_Num 54 Loss 0.011840126477181911\n",
      "Epoch 3 Batch_Num 55 Loss 0.011789318174123764\n",
      "Epoch 3 Batch_Num 56 Loss 0.014295128174126148\n",
      "Epoch 3 Batch_Num 57 Loss 0.00830465741455555\n",
      "Epoch 3 Batch_Num 58 Loss 0.004201061092317104\n",
      "Epoch 3 Batch_Num 59 Loss 0.013664412312209606\n",
      "Epoch 3 Batch_Num 60 Loss 0.0076651680283248425\n",
      "Epoch 3 Batch_Num 61 Loss 0.008417963981628418\n",
      "Epoch 3 Batch_Num 62 Loss 0.010718613862991333\n",
      "Epoch 3 Batch_Num 63 Loss 0.007553997915238142\n",
      "Epoch 3 Batch_Num 64 Loss 0.010841356590390205\n",
      "Epoch 3 Batch_Num 65 Loss 0.009887265972793102\n",
      "Epoch 3 Batch_Num 66 Loss 0.0141059635207057\n",
      "Epoch 3 Batch_Num 67 Loss 0.013860883191227913\n",
      "Epoch 3 Batch_Num 68 Loss 0.005976861342787743\n",
      "Epoch 3 Batch_Num 69 Loss 0.011777186766266823\n",
      "Epoch 3 Batch_Num 70 Loss 0.007112196180969477\n",
      "Epoch 3 Batch_Num 71 Loss 0.006857635919004679\n",
      "Epoch 3 Batch_Num 72 Loss 0.005945346783846617\n",
      "Epoch 3 Batch_Num 73 Loss 0.0022379581350833178\n",
      "Epoch 3 Batch_Num 74 Loss 0.011844845488667488\n",
      "Epoch 3 Batch_Num 75 Loss 0.011491731740534306\n",
      "Epoch 3 Batch_Num 76 Loss 0.004495141562074423\n",
      "Epoch 3 Batch_Num 77 Loss 0.004965892061591148\n",
      "Epoch 3 Batch_Num 78 Loss 0.020776038989424706\n",
      "Epoch 3 Batch_Num 79 Loss 0.006489007268100977\n",
      "Epoch 3 Batch_Num 80 Loss 0.013393337838351727\n",
      "Epoch 3 Batch_Num 81 Loss 0.008890769444406033\n",
      "Epoch 3 Batch_Num 82 Loss 0.005188909359276295\n",
      "Epoch 3 Batch_Num 83 Loss 0.0034580843057483435\n",
      "Epoch 3 Batch_Num 84 Loss 0.011311042122542858\n",
      "Epoch 3 Batch_Num 85 Loss 0.004585002548992634\n",
      "Epoch 3 Batch_Num 86 Loss 0.008224782533943653\n",
      "Epoch 3 Batch_Num 87 Loss 0.006494303233921528\n",
      "Epoch 3 Batch_Num 88 Loss 0.00956759974360466\n",
      "Epoch 3 Batch_Num 89 Loss 0.0014273897977545857\n",
      "Epoch 3 Batch_Num 90 Loss 0.012443776242434978\n",
      "Epoch 3 Batch_Num 91 Loss 0.004543330054730177\n",
      "Epoch 3 Batch_Num 92 Loss 0.00904535036534071\n",
      "Epoch 3 Batch_Num 93 Loss 0.00935987289994955\n",
      "Epoch 3 Batch_Num 94 Loss 0.006027787458151579\n",
      "Epoch 3 Batch_Num 95 Loss 0.003752791089937091\n",
      "Epoch 3 Batch_Num 96 Loss 0.01247936300933361\n",
      "Epoch 3 Batch_Num 97 Loss 0.005616563837975264\n",
      "Epoch 3 Batch_Num 98 Loss 0.008759429678320885\n",
      "Epoch 3 Batch_Num 99 Loss 0.008603676222264767\n",
      "Epoch 3 Batch_Num 100 Loss 0.011029650457203388\n",
      "Epoch 3 Batch_Num 101 Loss 0.010363420471549034\n",
      "Epoch 3 Batch_Num 102 Loss 0.0025701492559164762\n",
      "Epoch 3 Batch_Num 103 Loss 0.02545364573597908\n",
      "Epoch 3 Batch_Num 104 Loss 0.013774082064628601\n",
      "Epoch 3 Batch_Num 105 Loss 0.010978702455759048\n",
      "Epoch 3 Batch_Num 106 Loss 0.006196425296366215\n",
      "Epoch 3 Batch_Num 107 Loss 0.01664237678050995\n",
      "Epoch 3 Batch_Num 108 Loss 0.00735001964494586\n",
      "Epoch 3 Batch_Num 109 Loss 0.011384314857423306\n",
      "Epoch 3 Batch_Num 110 Loss 0.007833032868802547\n",
      "Epoch 3 Batch_Num 111 Loss 0.01548468042165041\n",
      "Epoch 3 Batch_Num 112 Loss 0.011216923594474792\n",
      "Epoch 3 Batch_Num 113 Loss 0.006010239478200674\n",
      "Epoch 3 Batch_Num 114 Loss 0.009316162206232548\n",
      "Epoch 3 Batch_Num 115 Loss 0.011976882815361023\n",
      "Epoch 3 Batch_Num 116 Loss 0.008626011200249195\n",
      "Epoch 3 Batch_Num 117 Loss 0.0068879397585988045\n",
      "Epoch 3 Batch_Num 118 Loss 0.0028189136646687984\n",
      "Epoch 3 Batch_Num 119 Loss 0.007593684829771519\n",
      "Epoch 3 Batch_Num 120 Loss 0.008299186825752258\n",
      "Epoch 3 Batch_Num 121 Loss 0.007417915854603052\n",
      "Epoch 3 Batch_Num 122 Loss 0.0057646604254841805\n",
      "Epoch 3 Batch_Num 123 Loss 0.008671528659760952\n",
      "Epoch 3 Batch_Num 124 Loss 0.011387872509658337\n",
      "Epoch 3 Batch_Num 125 Loss 0.008691824041306973\n",
      "Epoch 3 Batch_Num 126 Loss 0.014474085532128811\n",
      "Epoch 3 Batch_Num 127 Loss 0.004927570465952158\n",
      "Epoch 3 Batch_Num 128 Loss 0.01031879149377346\n",
      "Epoch 3 Batch_Num 129 Loss 0.0034518470056355\n",
      "Epoch 3 Batch_Num 130 Loss 0.007309182081371546\n",
      "Epoch 3 Batch_Num 131 Loss 0.005920831114053726\n",
      "Epoch 3 Batch_Num 132 Loss 0.0021831211633980274\n",
      "Epoch 3 Batch_Num 133 Loss 0.0031760961282998323\n",
      "Epoch 3 Batch_Num 134 Loss 0.019695861265063286\n",
      "Epoch 3 Batch_Num 135 Loss 0.01534153800457716\n",
      "Epoch 3 Batch_Num 136 Loss 0.013769167475402355\n",
      "Epoch 3 Batch_Num 137 Loss 0.01638268493115902\n",
      "Epoch 3 Batch_Num 138 Loss 0.006225533317774534\n",
      "Epoch 3 Batch_Num 139 Loss 0.00829467736184597\n",
      "Epoch 3 Batch_Num 140 Loss 0.006475348025560379\n",
      "Epoch 3 Batch_Num 141 Loss 0.0067533389665186405\n",
      "Epoch 3 Batch_Num 142 Loss 0.014290140941739082\n",
      "Epoch 3 Batch_Num 143 Loss 0.005198158323764801\n",
      "Epoch 3 Batch_Num 144 Loss 0.00695726228877902\n",
      "Epoch 3 Batch_Num 145 Loss 0.00537536246702075\n",
      "Epoch 3 Batch_Num 146 Loss 0.012229206040501595\n",
      "Epoch 3 Batch_Num 147 Loss 0.019526993855834007\n",
      "Epoch 3 Batch_Num 148 Loss 0.002778919879347086\n",
      "Epoch 3 Batch_Num 149 Loss 0.013965042307972908\n",
      "Epoch 3 Batch_Num 150 Loss 0.007236800622195005\n",
      "Epoch 3 Batch_Num 151 Loss 0.006365274544805288\n",
      "Epoch 3 Batch_Num 152 Loss 0.002919623628258705\n",
      "Epoch 3 Batch_Num 153 Loss 0.018437955528497696\n",
      "Epoch 3 Batch_Num 154 Loss 0.00858513917773962\n",
      "Epoch 3 Batch_Num 155 Loss 0.010056301020085812\n",
      "Epoch 3 Batch_Num 156 Loss 0.009473145939409733\n",
      "Epoch 3 Batch_Num 157 Loss 0.013918540440499783\n",
      "Epoch 3 Batch_Num 158 Loss 0.005941609852015972\n",
      "Epoch 3 Batch_Num 159 Loss 0.0039407736621797085\n",
      "Epoch 3 Batch_Num 160 Loss 0.006215788424015045\n",
      "Epoch 3 Batch_Num 161 Loss 0.013644428923726082\n",
      "Epoch 3 Batch_Num 162 Loss 0.0068575069308280945\n",
      "Epoch 3 Batch_Num 163 Loss 0.007902580313384533\n",
      "Epoch 3 Batch_Num 164 Loss 0.007215879391878843\n",
      "Epoch 3 Batch_Num 165 Loss 0.005845829378813505\n",
      "Epoch 3 Batch_Num 166 Loss 0.011800683103501797\n",
      "Epoch 3 Batch_Num 167 Loss 0.009466101415455341\n",
      "Epoch 3 Batch_Num 168 Loss 0.013186022639274597\n",
      "Epoch 3 Batch_Num 169 Loss 0.007601800840348005\n",
      "Epoch 3 Batch_Num 170 Loss 0.006848962511867285\n",
      "Epoch 3 Batch_Num 171 Loss 0.008846919052302837\n",
      "Epoch 3 Batch_Num 172 Loss 0.007810172624886036\n",
      "Epoch 3 Batch_Num 173 Loss 0.010461959056556225\n",
      "Epoch 3 Batch_Num 174 Loss 0.004151906818151474\n",
      "Epoch 3 Batch_Num 175 Loss 0.004472311120480299\n",
      "Epoch 3 Batch_Num 176 Loss 0.006466375198215246\n",
      "Epoch 3 Batch_Num 177 Loss 0.009841280989348888\n",
      "Epoch 3 Batch_Num 178 Loss 0.008615486323833466\n",
      "Epoch 3 Batch_Num 179 Loss 0.011071124114096165\n",
      "Epoch 3 Batch_Num 180 Loss 0.006414397154003382\n",
      "Epoch 3 Batch_Num 181 Loss 0.00813033152371645\n",
      "Epoch 3 Batch_Num 182 Loss 0.005569510627537966\n",
      "Epoch 3 Batch_Num 183 Loss 0.007687100674957037\n",
      "Epoch 3 Batch_Num 184 Loss 0.013975197449326515\n",
      "Epoch 3 Batch_Num 185 Loss 0.011781521141529083\n",
      "Epoch 3 Batch_Num 186 Loss 0.008546051569283009\n",
      "Epoch 3 Batch_Num 187 Loss 0.006549148354679346\n",
      "Epoch 3 Batch_Num 188 Loss 0.010121266357600689\n",
      "Epoch 3 Batch_Num 189 Loss 0.0019327898044139147\n",
      "Epoch 3 Batch_Num 190 Loss 0.0021422235295176506\n",
      "Epoch 3 Batch_Num 191 Loss 0.022982478141784668\n",
      "Epoch 3 Batch_Num 192 Loss 0.009122015908360481\n",
      "Epoch 3 Batch_Num 193 Loss 0.014753726311028004\n",
      "Epoch 3 Batch_Num 194 Loss 0.014799276366829872\n",
      "Epoch 3 Batch_Num 195 Loss 0.007213244680315256\n",
      "Epoch 3 Batch_Num 196 Loss 0.02036881260573864\n",
      "Epoch 3 Batch_Num 197 Loss 0.0032499965745955706\n",
      "Epoch 3 Batch_Num 198 Loss 0.0031077067833393812\n",
      "Epoch 3 Batch_Num 199 Loss 0.0023288768716156483\n",
      "Epoch 3 Batch_Num 200 Loss 0.007687360979616642\n",
      "Epoch 3 Batch_Num 201 Loss 0.008087552152574062\n",
      "Epoch 3 Batch_Num 202 Loss 0.014450738206505775\n",
      "Epoch 3 Batch_Num 203 Loss 0.012359206564724445\n",
      "Epoch 3 Batch_Num 204 Loss 0.010238285176455975\n",
      "Epoch 3 Batch_Num 205 Loss 0.008832325227558613\n",
      "Epoch 3 Batch_Num 206 Loss 0.01668895222246647\n",
      "Epoch 3 Batch_Num 207 Loss 0.007574261631816626\n",
      "Epoch 3 Batch_Num 208 Loss 0.006121777463704348\n",
      "Epoch 3 Batch_Num 209 Loss 0.012635123915970325\n",
      "Epoch 3 Batch_Num 210 Loss 0.013303354382514954\n",
      "Epoch 3 Batch_Num 211 Loss 0.014897572807967663\n",
      "Epoch 3 Batch_Num 212 Loss 0.0034625958651304245\n",
      "Epoch 3 Batch_Num 213 Loss 0.004816722124814987\n",
      "Epoch 3 Batch_Num 214 Loss 0.009100863710045815\n",
      "Epoch 3 Batch_Num 215 Loss 0.0036470608320087194\n",
      "Epoch 3 Batch_Num 216 Loss 0.012429040856659412\n",
      "Epoch 3 Batch_Num 217 Loss 0.012380992993712425\n",
      "Epoch 3 Batch_Num 218 Loss 0.006892807316035032\n",
      "Epoch 3 Batch_Num 219 Loss 0.006031381897628307\n",
      "Epoch 3 Batch_Num 220 Loss 0.00793191697448492\n",
      "Epoch 3 Batch_Num 221 Loss 0.006949049886316061\n",
      "Epoch 3 Batch_Num 222 Loss 0.006114365067332983\n",
      "Epoch 3 Batch_Num 223 Loss 0.0041334438137710094\n",
      "Epoch 3 Batch_Num 224 Loss 0.009819036349654198\n",
      "Epoch 3 Batch_Num 225 Loss 0.01384560763835907\n",
      "Epoch 3 Batch_Num 226 Loss 0.011922280304133892\n",
      "Epoch 3 Batch_Num 227 Loss 0.000592288444750011\n",
      "Epoch 3 Batch_Num 228 Loss 0.0032246813643723726\n",
      "Epoch 3 Batch_Num 229 Loss 0.004841625224798918\n",
      "Epoch 3 Batch_Num 230 Loss 8.841439557727426e-05\n",
      "Epoch 3 Batch_Num 231 Loss 0.004821693990379572\n",
      "Epoch 3 Batch_Num 232 Loss 0.003506419016048312\n",
      "Epoch 3 Batch_Num 233 Loss 0.016222383826971054\n",
      "Epoch 3 Batch_Num 234 Loss 0.02256632223725319\n",
      "Epoch 4 Batch_Num 0 Loss 0.011936238966882229\n",
      "Epoch 4 Batch_Num 1 Loss 0.01145769190043211\n",
      "Epoch 4 Batch_Num 2 Loss 0.009161149151623249\n",
      "Epoch 4 Batch_Num 3 Loss 0.018095150589942932\n",
      "Epoch 4 Batch_Num 4 Loss 0.009067653678357601\n",
      "Epoch 4 Batch_Num 5 Loss 0.005964513868093491\n",
      "Epoch 4 Batch_Num 6 Loss 0.011243815533816814\n",
      "Epoch 4 Batch_Num 7 Loss 0.0022570383735001087\n",
      "Epoch 4 Batch_Num 8 Loss 0.0032680972944945097\n",
      "Epoch 4 Batch_Num 9 Loss 0.006351382937282324\n",
      "Epoch 4 Batch_Num 10 Loss 0.010965459980070591\n",
      "Epoch 4 Batch_Num 11 Loss 0.008902902714908123\n",
      "Epoch 4 Batch_Num 12 Loss 0.002792294602841139\n",
      "Epoch 4 Batch_Num 13 Loss 0.007855000905692577\n",
      "Epoch 4 Batch_Num 14 Loss 0.00985431857407093\n",
      "Epoch 4 Batch_Num 15 Loss 0.004367662128061056\n",
      "Epoch 4 Batch_Num 16 Loss 0.005514227319508791\n",
      "Epoch 4 Batch_Num 17 Loss 0.008323985151946545\n",
      "Epoch 4 Batch_Num 18 Loss 0.0046854810789227486\n",
      "Epoch 4 Batch_Num 19 Loss 0.00831344909965992\n",
      "Epoch 4 Batch_Num 20 Loss 0.009175293147563934\n",
      "Epoch 4 Batch_Num 21 Loss 0.002889483468607068\n",
      "Epoch 4 Batch_Num 22 Loss 0.011393934488296509\n",
      "Epoch 4 Batch_Num 23 Loss 0.0038738492876291275\n",
      "Epoch 4 Batch_Num 24 Loss 0.00732808280736208\n",
      "Epoch 4 Batch_Num 25 Loss 0.005678938701748848\n",
      "Epoch 4 Batch_Num 26 Loss 0.007047014776617289\n",
      "Epoch 4 Batch_Num 27 Loss 0.010808265767991543\n",
      "Epoch 4 Batch_Num 28 Loss 0.008207852020859718\n",
      "Epoch 4 Batch_Num 29 Loss 0.004345564637333155\n",
      "Epoch 4 Batch_Num 30 Loss 0.00623572850599885\n",
      "Epoch 4 Batch_Num 31 Loss 0.004640554543584585\n",
      "Epoch 4 Batch_Num 32 Loss 0.010261430405080318\n",
      "Epoch 4 Batch_Num 33 Loss 0.0047533344477415085\n",
      "Epoch 4 Batch_Num 34 Loss 0.01459444034844637\n",
      "Epoch 4 Batch_Num 35 Loss 0.005423737224191427\n",
      "Epoch 4 Batch_Num 36 Loss 0.0060582831501960754\n",
      "Epoch 4 Batch_Num 37 Loss 0.005627456586807966\n",
      "Epoch 4 Batch_Num 38 Loss 0.00374963297508657\n",
      "Epoch 4 Batch_Num 39 Loss 0.00430314103141427\n",
      "Epoch 4 Batch_Num 40 Loss 0.013378246687352657\n",
      "Epoch 4 Batch_Num 41 Loss 0.0018272221786901355\n",
      "Epoch 4 Batch_Num 42 Loss 0.012763219885528088\n",
      "Epoch 4 Batch_Num 43 Loss 0.003835411509498954\n",
      "Epoch 4 Batch_Num 44 Loss 0.0014199273427948356\n",
      "Epoch 4 Batch_Num 45 Loss 0.008995305746793747\n",
      "Epoch 4 Batch_Num 46 Loss 0.009103861637413502\n",
      "Epoch 4 Batch_Num 47 Loss 0.0030602230690419674\n",
      "Epoch 4 Batch_Num 48 Loss 0.006092930678278208\n",
      "Epoch 4 Batch_Num 49 Loss 0.010181293822824955\n",
      "Epoch 4 Batch_Num 50 Loss 0.0066131772473454475\n",
      "Epoch 4 Batch_Num 51 Loss 0.012158572673797607\n",
      "Epoch 4 Batch_Num 52 Loss 0.005082004237920046\n",
      "Epoch 4 Batch_Num 53 Loss 0.008708047680556774\n",
      "Epoch 4 Batch_Num 54 Loss 0.009333230555057526\n",
      "Epoch 4 Batch_Num 55 Loss 0.008340871892869473\n",
      "Epoch 4 Batch_Num 56 Loss 0.016213808208703995\n",
      "Epoch 4 Batch_Num 57 Loss 0.004380625672638416\n",
      "Epoch 4 Batch_Num 58 Loss 0.0017643027240410447\n",
      "Epoch 4 Batch_Num 59 Loss 0.00949141662567854\n",
      "Epoch 4 Batch_Num 60 Loss 0.005011314060539007\n",
      "Epoch 4 Batch_Num 61 Loss 0.007372629828751087\n",
      "Epoch 4 Batch_Num 62 Loss 0.005687779746949673\n",
      "Epoch 4 Batch_Num 63 Loss 0.006661070976406336\n",
      "Epoch 4 Batch_Num 64 Loss 0.004172428976744413\n",
      "Epoch 4 Batch_Num 65 Loss 0.007181602995842695\n",
      "Epoch 4 Batch_Num 66 Loss 0.0075168064795434475\n",
      "Epoch 4 Batch_Num 67 Loss 0.01053824182599783\n",
      "Epoch 4 Batch_Num 68 Loss 0.0033439849503338337\n",
      "Epoch 4 Batch_Num 69 Loss 0.0082963602617383\n",
      "Epoch 4 Batch_Num 70 Loss 0.004073874559253454\n",
      "Epoch 4 Batch_Num 71 Loss 0.006453868933022022\n",
      "Epoch 4 Batch_Num 72 Loss 0.005133855156600475\n",
      "Epoch 4 Batch_Num 73 Loss 0.0033404899295419455\n",
      "Epoch 4 Batch_Num 74 Loss 0.008040252141654491\n",
      "Epoch 4 Batch_Num 75 Loss 0.006681253667920828\n",
      "Epoch 4 Batch_Num 76 Loss 0.0032009833957999945\n",
      "Epoch 4 Batch_Num 77 Loss 0.005191867705434561\n",
      "Epoch 4 Batch_Num 78 Loss 0.022451933473348618\n",
      "Epoch 4 Batch_Num 79 Loss 0.004078072961419821\n",
      "Epoch 4 Batch_Num 80 Loss 0.007660535164177418\n",
      "Epoch 4 Batch_Num 81 Loss 0.0042358217760920525\n",
      "Epoch 4 Batch_Num 82 Loss 0.00671399524435401\n",
      "Epoch 4 Batch_Num 83 Loss 0.0012160016922280192\n",
      "Epoch 4 Batch_Num 84 Loss 0.010545735247433186\n",
      "Epoch 4 Batch_Num 85 Loss 0.006530150771141052\n",
      "Epoch 4 Batch_Num 86 Loss 0.003437914652749896\n",
      "Epoch 4 Batch_Num 87 Loss 0.005628752987831831\n",
      "Epoch 4 Batch_Num 88 Loss 0.005554675590246916\n",
      "Epoch 4 Batch_Num 89 Loss 0.0012489374494180083\n",
      "Epoch 4 Batch_Num 90 Loss 0.010207274928689003\n",
      "Epoch 4 Batch_Num 91 Loss 0.001815209980122745\n",
      "Epoch 4 Batch_Num 92 Loss 0.011691044084727764\n",
      "Epoch 4 Batch_Num 93 Loss 0.0063751875422894955\n",
      "Epoch 4 Batch_Num 94 Loss 0.0030059260316193104\n",
      "Epoch 4 Batch_Num 95 Loss 0.003614176530390978\n",
      "Epoch 4 Batch_Num 96 Loss 0.009200175292789936\n",
      "Epoch 4 Batch_Num 97 Loss 0.004167745355516672\n",
      "Epoch 4 Batch_Num 98 Loss 0.004246740601956844\n",
      "Epoch 4 Batch_Num 99 Loss 0.008285143412649632\n",
      "Epoch 4 Batch_Num 100 Loss 0.0052717747166752815\n",
      "Epoch 4 Batch_Num 101 Loss 0.00524950074031949\n",
      "Epoch 4 Batch_Num 102 Loss 0.0029218101408332586\n",
      "Epoch 4 Batch_Num 103 Loss 0.01547183096408844\n",
      "Epoch 4 Batch_Num 104 Loss 0.011331446468830109\n",
      "Epoch 4 Batch_Num 105 Loss 0.009635701775550842\n",
      "Epoch 4 Batch_Num 106 Loss 0.006338921375572681\n",
      "Epoch 4 Batch_Num 107 Loss 0.009033958427608013\n",
      "Epoch 4 Batch_Num 108 Loss 0.007082553114742041\n",
      "Epoch 4 Batch_Num 109 Loss 0.013121175579726696\n",
      "Epoch 4 Batch_Num 110 Loss 0.0078488914296031\n",
      "Epoch 4 Batch_Num 111 Loss 0.012362809851765633\n",
      "Epoch 4 Batch_Num 112 Loss 0.009988543577492237\n",
      "Epoch 4 Batch_Num 113 Loss 0.0093306889757514\n",
      "Epoch 4 Batch_Num 114 Loss 0.005664639174938202\n",
      "Epoch 4 Batch_Num 115 Loss 0.0066102491691708565\n",
      "Epoch 4 Batch_Num 116 Loss 0.006069079041481018\n",
      "Epoch 4 Batch_Num 117 Loss 0.007670154329389334\n",
      "Epoch 4 Batch_Num 118 Loss 0.0034543261863291264\n",
      "Epoch 4 Batch_Num 119 Loss 0.00905300211161375\n",
      "Epoch 4 Batch_Num 120 Loss 0.009851234033703804\n",
      "Epoch 4 Batch_Num 121 Loss 0.006229925900697708\n",
      "Epoch 4 Batch_Num 122 Loss 0.003471188712865114\n",
      "Epoch 4 Batch_Num 123 Loss 0.006803411990404129\n",
      "Epoch 4 Batch_Num 124 Loss 0.006795425899326801\n",
      "Epoch 4 Batch_Num 125 Loss 0.0075537352822721004\n",
      "Epoch 4 Batch_Num 126 Loss 0.01277479063719511\n",
      "Epoch 4 Batch_Num 127 Loss 0.004430919885635376\n",
      "Epoch 4 Batch_Num 128 Loss 0.005902803968638182\n",
      "Epoch 4 Batch_Num 129 Loss 0.002950826194137335\n",
      "Epoch 4 Batch_Num 130 Loss 0.0070642754435539246\n",
      "Epoch 4 Batch_Num 131 Loss 0.004572057165205479\n",
      "Epoch 4 Batch_Num 132 Loss 0.0022824190091341734\n",
      "Epoch 4 Batch_Num 133 Loss 0.002723009092733264\n",
      "Epoch 4 Batch_Num 134 Loss 0.014989063143730164\n",
      "Epoch 4 Batch_Num 135 Loss 0.009023969061672688\n",
      "Epoch 4 Batch_Num 136 Loss 0.005498179700225592\n",
      "Epoch 4 Batch_Num 137 Loss 0.010066650807857513\n",
      "Epoch 4 Batch_Num 138 Loss 0.005527525208890438\n",
      "Epoch 4 Batch_Num 139 Loss 0.013172115199267864\n",
      "Epoch 4 Batch_Num 140 Loss 0.006901117507368326\n",
      "Epoch 4 Batch_Num 141 Loss 0.006207531783729792\n",
      "Epoch 4 Batch_Num 142 Loss 0.008276154287159443\n",
      "Epoch 4 Batch_Num 143 Loss 0.007717008236795664\n",
      "Epoch 4 Batch_Num 144 Loss 0.005371701903641224\n",
      "Epoch 4 Batch_Num 145 Loss 0.002288609743118286\n",
      "Epoch 4 Batch_Num 146 Loss 0.00934050977230072\n",
      "Epoch 4 Batch_Num 147 Loss 0.016709771007299423\n",
      "Epoch 4 Batch_Num 148 Loss 0.002697134157642722\n",
      "Epoch 4 Batch_Num 149 Loss 0.010418304242193699\n",
      "Epoch 4 Batch_Num 150 Loss 0.006115639582276344\n",
      "Epoch 4 Batch_Num 151 Loss 0.002602111315354705\n",
      "Epoch 4 Batch_Num 152 Loss 0.002555146813392639\n",
      "Epoch 4 Batch_Num 153 Loss 0.02318660356104374\n",
      "Epoch 4 Batch_Num 154 Loss 0.005827932618558407\n",
      "Epoch 4 Batch_Num 155 Loss 0.00714879622682929\n",
      "Epoch 4 Batch_Num 156 Loss 0.011693062260746956\n",
      "Epoch 4 Batch_Num 157 Loss 0.00888002384454012\n",
      "Epoch 4 Batch_Num 158 Loss 0.006780756171792746\n",
      "Epoch 4 Batch_Num 159 Loss 0.005194576922804117\n",
      "Epoch 4 Batch_Num 160 Loss 0.004970321897417307\n",
      "Epoch 4 Batch_Num 161 Loss 0.01076110452413559\n",
      "Epoch 4 Batch_Num 162 Loss 0.00495124189183116\n",
      "Epoch 4 Batch_Num 163 Loss 0.005683829076588154\n",
      "Epoch 4 Batch_Num 164 Loss 0.002046660054475069\n",
      "Epoch 4 Batch_Num 165 Loss 0.005459305830299854\n",
      "Epoch 4 Batch_Num 166 Loss 0.004310509655624628\n",
      "Epoch 4 Batch_Num 167 Loss 0.006963962223380804\n",
      "Epoch 4 Batch_Num 168 Loss 0.01099043246358633\n",
      "Epoch 4 Batch_Num 169 Loss 0.007048825267702341\n",
      "Epoch 4 Batch_Num 170 Loss 0.005576435010880232\n",
      "Epoch 4 Batch_Num 171 Loss 0.006219765637069941\n",
      "Epoch 4 Batch_Num 172 Loss 0.0057634818367660046\n",
      "Epoch 4 Batch_Num 173 Loss 0.008966299705207348\n",
      "Epoch 4 Batch_Num 174 Loss 0.0009264363325200975\n",
      "Epoch 4 Batch_Num 175 Loss 0.0020275877323001623\n",
      "Epoch 4 Batch_Num 176 Loss 0.005267894361168146\n",
      "Epoch 4 Batch_Num 177 Loss 0.010176572017371655\n",
      "Epoch 4 Batch_Num 178 Loss 0.006297639105468988\n",
      "Epoch 4 Batch_Num 179 Loss 0.012247750535607338\n",
      "Epoch 4 Batch_Num 180 Loss 0.005495121702551842\n",
      "Epoch 4 Batch_Num 181 Loss 0.008796262554824352\n",
      "Epoch 4 Batch_Num 182 Loss 0.0020181764848530293\n",
      "Epoch 4 Batch_Num 183 Loss 0.004425426945090294\n",
      "Epoch 4 Batch_Num 184 Loss 0.017172921448946\n",
      "Epoch 4 Batch_Num 185 Loss 0.00417250394821167\n",
      "Epoch 4 Batch_Num 186 Loss 0.006059037055820227\n",
      "Epoch 4 Batch_Num 187 Loss 0.0033008612226694822\n",
      "Epoch 4 Batch_Num 188 Loss 0.010072745382785797\n",
      "Epoch 4 Batch_Num 189 Loss 0.0013614293420687318\n",
      "Epoch 4 Batch_Num 190 Loss 0.0008011372992768884\n",
      "Epoch 4 Batch_Num 191 Loss 0.013969749212265015\n",
      "Epoch 4 Batch_Num 192 Loss 0.007647953927516937\n",
      "Epoch 4 Batch_Num 193 Loss 0.010647417977452278\n",
      "Epoch 4 Batch_Num 194 Loss 0.01237289234995842\n",
      "Epoch 4 Batch_Num 195 Loss 0.0047157215885818005\n",
      "Epoch 4 Batch_Num 196 Loss 0.013861476443707943\n",
      "Epoch 4 Batch_Num 197 Loss 0.0029322931077331305\n",
      "Epoch 4 Batch_Num 198 Loss 0.0067421370185911655\n",
      "Epoch 4 Batch_Num 199 Loss 0.0019856176804751158\n",
      "Epoch 4 Batch_Num 200 Loss 0.01174485869705677\n",
      "Epoch 4 Batch_Num 201 Loss 0.002786173252388835\n",
      "Epoch 4 Batch_Num 202 Loss 0.01095158513635397\n",
      "Epoch 4 Batch_Num 203 Loss 0.009126359596848488\n",
      "Epoch 4 Batch_Num 204 Loss 0.012963324785232544\n",
      "Epoch 4 Batch_Num 205 Loss 0.005677998065948486\n",
      "Epoch 4 Batch_Num 206 Loss 0.010984878055751324\n",
      "Epoch 4 Batch_Num 207 Loss 0.0076401024125516415\n",
      "Epoch 4 Batch_Num 208 Loss 0.006605047732591629\n",
      "Epoch 4 Batch_Num 209 Loss 0.007110645063221455\n",
      "Epoch 4 Batch_Num 210 Loss 0.012135381810367107\n",
      "Epoch 4 Batch_Num 211 Loss 0.008770974352955818\n",
      "Epoch 4 Batch_Num 212 Loss 0.0037382168229669333\n",
      "Epoch 4 Batch_Num 213 Loss 0.0037376934196799994\n",
      "Epoch 4 Batch_Num 214 Loss 0.004007644485682249\n",
      "Epoch 4 Batch_Num 215 Loss 0.0024338960647583008\n",
      "Epoch 4 Batch_Num 216 Loss 0.008487789891660213\n",
      "Epoch 4 Batch_Num 217 Loss 0.009833735413849354\n",
      "Epoch 4 Batch_Num 218 Loss 0.006468395236879587\n",
      "Epoch 4 Batch_Num 219 Loss 0.00555318733677268\n",
      "Epoch 4 Batch_Num 220 Loss 0.0035433003213256598\n",
      "Epoch 4 Batch_Num 221 Loss 0.005910595878958702\n",
      "Epoch 4 Batch_Num 222 Loss 0.008220496587455273\n",
      "Epoch 4 Batch_Num 223 Loss 0.001973471837118268\n",
      "Epoch 4 Batch_Num 224 Loss 0.0044644419103860855\n",
      "Epoch 4 Batch_Num 225 Loss 0.005879982840269804\n",
      "Epoch 4 Batch_Num 226 Loss 0.0061768717132508755\n",
      "Epoch 4 Batch_Num 227 Loss 0.001972799887880683\n",
      "Epoch 4 Batch_Num 228 Loss 0.00046429294161498547\n",
      "Epoch 4 Batch_Num 229 Loss 0.002153943758457899\n",
      "Epoch 4 Batch_Num 230 Loss 0.00039752674638293684\n",
      "Epoch 4 Batch_Num 231 Loss 0.006651667412370443\n",
      "Epoch 4 Batch_Num 232 Loss 0.0019029846880584955\n",
      "Epoch 4 Batch_Num 233 Loss 0.01131343375891447\n",
      "Epoch 4 Batch_Num 234 Loss 0.01693977415561676\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for f in range(num_epochs):\n",
    "    for batch_num, minibatch in enumerate(train_loader):\n",
    "        minibatch_x, minibatch_y = minibatch[0], minibatch[1]\n",
    "        \n",
    "        output = model.forward(torch.Tensor(minibatch_x.float()).cuda())\n",
    "        loss = criterion(output, torch.Tensor(minibatch_y.float()).cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {f} Batch_Num {batch_num} Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de231006-049d-41ae-9838-2e68cd2711f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/07 10:39:09 INFO mlflow.tracking.fluent: Experiment with name 'PyTorch_MNIST' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_acc:  0.9786\n",
      "auc_score:  0.9881924592706973\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"PyTorch_MNIST\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    preds = model.forward(torch.Tensor(x_test.float()).cuda())\n",
    "    preds = np.round(preds.detach().cpu().numpy())\n",
    "    \n",
    "    eval_acc = accuracy_score(y_test, preds)\n",
    "    auc_score = roc_auc_score(y_test, preds)\n",
    "    \n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    \n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"auc_score\", auc_score)\n",
    "    \n",
    "    print(\"eval_acc: \", eval_acc)\n",
    "    print(\"auc_score: \", auc_score)\n",
    "    \n",
    "    mlflow.pytorch.log_model(model, \"PyTorch_MNIST\")\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
